{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bert_Adapter.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "QN3o7PY3YEyL",
        "SXd9NmiXYI5E",
        "Hb8Gx6YOYVUq",
        "wtgLiE1aYflG"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3sSHa16uoAIU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b0a6dbf-dd3f-4af5-b410-79a093f0c59d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['SMSSpamCollection.txt',\n",
              " '垃圾邮件分类.ipynb',\n",
              " 'vocab.txt',\n",
              " 'bert-large-uncased',\n",
              " 'bert-base-uncased',\n",
              " 'SMSSpamCollection.csv']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "path = \"/content/drive/My Drive/Spam-classification-master/\"\n",
        "\n",
        "os.chdir(path)\n",
        "os.listdir(path)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data"
      ],
      "metadata": {
        "id": "QN3o7PY3YEyL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ham：非垃圾短信\n",
        "# spam：垃圾短信\n",
        "# \\t键后面是短信的正文\n",
        "\n",
        "# 2.导入要用的包\n",
        "import pandas as pd \n",
        "from sklearn import linear_model\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer # sklearn包中，特殊提取中的文本模块中，特殊字符向量化方法\n",
        "\n",
        "# 3.读入数据集\n",
        "path = './'\n",
        "filename = 'SMSSpamCollection.txt'\n",
        "df = pd.read_csv(path + filename, delimiter='\\t', header=None)# 用\\t分割，没有文件头\n",
        "# 生成label和x输入\n",
        "y,X_train = df[0],df[1]"
      ],
      "metadata": {
        "id": "trnWE7wfHLhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch_pretrained_bert"
      ],
      "metadata": {
        "id": "CY2rK9EKnojL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "SXd9NmiXYI5E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import re\n",
        "from math import sqrt as msqrt\n",
        "\n",
        "import torch\n",
        "import torch.functional as F\n",
        "from torch import nn\n",
        "from torch.optim import Adadelta\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "#from pytorch_pretrained_bert import BertModel, BertTokenizer\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "TwFjdWr5nl2f"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len = 50\n",
        "max_vocab = 30522\n",
        "max_pred = 5\n",
        "\n",
        "d_k = d_v = 64\n",
        "d_model = 768  # n_heads * d_k\n",
        "d_ff = d_model * 4\n",
        "\n",
        "n_heads = 12\n",
        "n_layers = 12\n",
        "n_segs = 2\n",
        "\n",
        "p_dropout = .1\n",
        "# BERT propability defined\n",
        "p_mask = .8\n",
        "p_replace = .1\n",
        "p_do_nothing = 1 - p_mask - p_replace\n",
        "\n",
        "# adapter\n",
        "hidden_size=64\n",
        "init_scale=1e-3\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device = torch.device(device)"
      ],
      "metadata": {
        "id": "dWGd33pTke83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pad_mask(tokens, pad_idx=0):\n",
        "    '''\n",
        "    suppose index of [PAD] is zero in word2idx\n",
        "    tokens: [batch, seq_len]\n",
        "    '''\n",
        "    batch, seq_len = tokens.size()\n",
        "    pad_mask = tokens.data.eq(pad_idx).unsqueeze(1)\n",
        "    pad_mask = pad_mask.expand(batch, seq_len, seq_len)\n",
        "    return pad_mask\n",
        "\n",
        "class Embeddings(nn.Module):\n",
        "    \"\"\"\n",
        "    Word Embedding, Position Embedding, Segment Embedding\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super(Embeddings, self).__init__()\n",
        "        self.seg_emb = nn.Embedding(n_segs, d_model)\n",
        "        self.word_emb = nn.Embedding(max_vocab, d_model)\n",
        "\n",
        "        self.pos_emb = nn.Embedding(max_len, d_model)\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(p_dropout)\n",
        "\n",
        "    def forward(self, x, seg):\n",
        "        '''\n",
        "        x: [batch, seq_len]\n",
        "        '''\n",
        "        \n",
        "\n",
        "        # positional embedding\n",
        "        pos = torch.arange(x.shape[1], dtype=torch.long, device=device)\n",
        "        pos = pos.unsqueeze(0).expand_as(x)\n",
        "        pos_enc = self.pos_emb(pos)\n",
        "        \n",
        "        word_enc = self.word_emb(x)\n",
        "        \n",
        "        seg_enc = self.seg_emb(seg)\n",
        "        x = self.norm(word_enc + pos_enc + seg_enc)\n",
        "        return self.dropout(x)\n",
        "        # return: [batch, seq_len, d_model]\n",
        "\n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ScaledDotProductAttention, self).__init__()\n",
        "\n",
        "    def forward(self, Q, K, V, attn_mask):\n",
        "        scores = torch.matmul(Q, K.transpose(-1, -2) / msqrt(d_k))\n",
        "        # scores: [batch, n_heads, seq_len, seq_len]\n",
        "  #      print(scores.shape)\n",
        "        print(attn_mask.shape)\n",
        "        \n",
        "        #要先用mask替换掉\n",
        "        scores.masked_fill_(attn_mask, -1e9)\n",
        "        attn = nn.Softmax(dim=-1)(scores)\n",
        "        # context: [batch, n_heads, seq_len, d_v]\n",
        "        context = torch.matmul(attn, V)\n",
        "        return context\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.W_Q = nn.Linear(d_model, d_k * n_heads, bias=False)\n",
        "        self.W_K = nn.Linear(d_model, d_k * n_heads, bias=False)\n",
        "        self.W_V = nn.Linear(d_model, d_v * n_heads, bias=False)\n",
        "        self.fc = nn.Linear(n_heads * d_v, d_model, bias=False)\n",
        "\n",
        "    def forward(self, Q, K, V, attn_mask):\n",
        "        '''\n",
        "        Q, K, V: [batch, seq_len, d_model]\n",
        "        attn_mask: [batch, seq_len, seq_len]\n",
        "        '''\n",
        "        batch = Q.size(0)\n",
        "        '''\n",
        "        split Q, K, V to per head formula: [batch, seq_len, n_heads, d_k]\n",
        "        Convenient for matrix multiply opearation later\n",
        "        q, k, v: [batch, n_heads, seq_len, d_k / d_v]\n",
        "        '''\n",
        "        per_Q = self.W_Q(Q).view(batch, -1, n_heads, d_k).transpose(1, 2)\n",
        "        per_K = self.W_K(K).view(batch, -1, n_heads, d_k).transpose(1, 2)\n",
        "        per_V = self.W_V(V).view(batch, -1, n_heads, d_v).transpose(1, 2)\n",
        "   #     print('hi')\n",
        "   #     print(attn_mask.shape)\n",
        "        attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1)\n",
        "   #     print(attn_mask.shape)\n",
        "        # context: [batch, n_heads, seq_len, d_v]\n",
        "        context = ScaledDotProductAttention()(per_Q, per_K, per_V, attn_mask)\n",
        "        context = context.transpose(1, 2).contiguous().view(\n",
        "            batch, -1, n_heads * d_v)\n",
        "\n",
        "        # output: [batch, seq_len, d_model]\n",
        "        output = self.fc(context)\n",
        "        return output\n",
        "\n",
        "\n",
        "def gelu(x):\n",
        "    '''\n",
        "    激活函数\n",
        "    Two way to implements GELU:\n",
        "    0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
        "    or\n",
        "    0.5 * x * (1. + torch.erf(torch.sqrt(x, 2)))\n",
        "    '''\n",
        "    return .5 * x * (1. + torch.erf(x / msqrt(2.)))\n",
        "\n",
        "\n",
        "class FeedForwardNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FeedForwardNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(p_dropout)\n",
        "        self.gelu = gelu\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "def truncated_normal_(tensor,mean=0,std=init_scale):\n",
        "    with torch.no_grad():\n",
        "        size = tensor.shape\n",
        "        tmp = tensor.new_empty(size+(4,)).normal_()\n",
        "        valid = (tmp < 2) & (tmp > -2)\n",
        "        ind = valid.max(-1, keepdim=True)[1]\n",
        "        tensor.data.copy_(tmp.gather(-1, ind).squeeze(-1))\n",
        "        tensor.data.mul_(std).add_(mean)\n",
        "        return tensor\n",
        "\n",
        "\n",
        "def feedforward_adapter(input_tensor):\n",
        "    in_size = input_tensor.shape[1]\n",
        "    w_1 = nn.Parameter(torch.Tensor(in_size,hidden_size*n_heads))\n",
        "    print(\"123\")\n",
        "    print(w_1.shape)\n",
        "    w_1 = truncated_normal_(w_1,mean=0,std=init_scale)\n",
        "    print(w_1.shape)\n",
        "    b_1 = nn.Parameter(torch.Tensor(1,hidden_size))\n",
        "    print(b_1.shape)\n",
        "    net = torch.tensordot(input_tensor, w_1, [[1], [0]])\n",
        "    net= net + b_1  #指定前面维度1和后面维度0做内积。\n",
        "    net = gelu(net)\n",
        "\n",
        "    w_2 = nn.Parameter(torch.Tensor(hidden_size*n_heads,in_size))\n",
        "    w_2 = truncated_normal_(w_2,mean=0,std=init_scale)\n",
        "    b_2 = nn.Parameter(torch.Tensor(1,in_size))\n",
        "    net = torch.tensordot(net, w_2, [[1], [0]]) + b_2\n",
        "\n",
        "        #残差链接\n",
        "    return net + input_tensor\n",
        "\n",
        "class Adapter(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Adapter,self).__init__()\n",
        "        self.a_1 = nn.Linear(768,d_model, bias=False)\n",
        "        self.a_2 = nn.Linear(d_model,768, bias=False)\n",
        "        self.gelu = gelu\n",
        "    def forward(self,input_tensor):\n",
        "        print(\"4\")\n",
        "        print(input_tensor.shape)\n",
        "        net=self.a_1(input_tensor)\n",
        "        print(net.shape)\n",
        "        net = self.gelu(net)\n",
        "        net=self.a_2(net)\n",
        "        return net + input_tensor\n",
        "\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.enc_attn = MultiHeadAttention()\n",
        "        self.adapter1 = Adapter()\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.ffn = FeedForwardNetwork()\n",
        "        self.adapter2 = Adapter()\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "\n",
        "    def forward(self, x, pad_mask):\n",
        "        '''\n",
        "        pre-norm\n",
        "        see more detail in https://openreview.net/pdf?id=B1x8anVFPr\n",
        "\n",
        "        x: [batch, seq_len, d_model]\n",
        "        '''\n",
        "        residual = x\n",
        "    #    print(x.shape)\n",
        "    #    print(\"12345\")\n",
        "    #    print(pad_mask.shape)\n",
        "        x = self.enc_attn(x, x, x, pad_mask)\n",
        "        print('hi')\n",
        "        print(x.shape)\n",
        "        x = self.adapter1(x)\n",
        "        print(residual.shape)\n",
        "        x = x+residual\n",
        "        x = self.norm1(x)\n",
        "        residual = x\n",
        "        \n",
        "        x=self.adapter2(x)\n",
        "        x = self.ffn(x)+ residual\n",
        "        x = self.norm2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Pooler(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Pooler, self).__init__()\n",
        "        self.fc = nn.Linear(d_model, d_model)\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "    def forward(self, x):\n",
        "        '''\n",
        "        x: [batch, d_model] (first place output)\n",
        "        '''\n",
        "        x = self.fc(x)\n",
        "        x = self.tanh(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "0Utf9gOKOsuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BERT(nn.Module):\n",
        "    def __init__(self, n_layers):\n",
        "        super(BERT, self).__init__()\n",
        "        self.embedding = Embeddings()\n",
        "        self.encoders = nn.ModuleList([\n",
        "            EncoderLayer() for _ in range(n_layers)\n",
        "        ])\n",
        "        self.pooler = Pooler()\n",
        "        self.gelu = gelu\n",
        "        self.classify = nn.Linear(d_model, 2)\n",
        "\n",
        "    def forward(self, tokens, segments,mask):\n",
        "        output = self.embedding(tokens, segments)\n",
        "        enc_self_pad_mask = get_pad_mask(tokens)\n",
        "    #    print(enc_self_pad_mask)\n",
        "        for layer in self.encoders:\n",
        "            output = layer(output, enc_self_pad_mask)\n",
        "        # output: [batch, max_len, d_model]\n",
        "\n",
        "        # NSP Task\n",
        "        hidden_pool = self.pooler(output[:, 0])\n",
        "        logits_cls = self.classify(hidden_pool)\n",
        "\n",
        "        return logits_cls"
      ],
      "metadata": {
        "id": "YeAWw5-Jqoi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchinfo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yb0bj8jPskNz",
        "outputId": "d502c02f-213f-4fa6-cfa6-80aad89a3461"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.6.5-py3-none-any.whl (21 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.6.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adaptermodel = BERT(n_layers)\n",
        "from torchinfo import summary\n",
        "#summary(adaptermodel)"
      ],
      "metadata": {
        "id": "67mUcZyisYvT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2lZn-zn7rImr",
        "outputId": "83019dee-ca5d-4eee-ba1f-c154c5a300ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bert-base-uncased   SMSSpamCollection.csv  vocab.txt\n",
            "bert-large-uncased  SMSSpamCollection.txt  垃圾邮件分类.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pytorch_pretrained_bert import BertModel, BertTokenizer\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# 加载bert模型，这个路径文件夹下有bert_config.json配置文件和model.bin模型权重文件\n",
        "bert = BertModel.from_pretrained('bert-base-uncased')\n"
      ],
      "metadata": {
        "id": "OnW_FSG7ZBac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb58433a-9629-4efc-8103-9c311ffafcf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 407873900/407873900 [00:09<00:00, 41254557.61B/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained_dict = bert.state_dict()\n"
      ],
      "metadata": {
        "id": "XRBOCM4ysJEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_dict = adaptermodel.state_dict()"
      ],
      "metadata": {
        "id": "eyhbfM-HsUUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained_dict =  {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
        "# 更新现有的model_dict\n",
        "model_dict.update(pretrained_dict)\n",
        "# 加载我们真正需要的state_dict\n",
        "adaptermodel.load_state_dict(model_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-tjkpe7tUPt",
        "outputId": "ab21f117-faf2-4005-a80e-f2ed322fca8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary(adaptermodel)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdvedOdltf0e",
        "outputId": "47d067c2-489c-4258-d551-82a5696cd573"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "=================================================================\n",
              "Layer (type:depth-idx)                   Param #\n",
              "=================================================================\n",
              "BERT                                     --\n",
              "├─Embeddings: 1-1                        --\n",
              "│    └─Embedding: 2-1                    1,536\n",
              "│    └─Embedding: 2-2                    23,440,896\n",
              "│    └─Embedding: 2-3                    38,400\n",
              "│    └─LayerNorm: 2-4                    1,536\n",
              "│    └─Dropout: 2-5                      --\n",
              "├─ModuleList: 1-2                        --\n",
              "│    └─EncoderLayer: 2-6                 --\n",
              "│    │    └─MultiHeadAttention: 3-1      2,359,296\n",
              "│    │    └─Adapter: 3-2                 1,179,648\n",
              "│    │    └─LayerNorm: 3-3               1,536\n",
              "│    │    └─FeedForwardNetwork: 3-4      4,722,432\n",
              "│    │    └─Adapter: 3-5                 1,179,648\n",
              "│    │    └─LayerNorm: 3-6               1,536\n",
              "│    └─EncoderLayer: 2-7                 --\n",
              "│    │    └─MultiHeadAttention: 3-7      2,359,296\n",
              "│    │    └─Adapter: 3-8                 1,179,648\n",
              "│    │    └─LayerNorm: 3-9               1,536\n",
              "│    │    └─FeedForwardNetwork: 3-10     4,722,432\n",
              "│    │    └─Adapter: 3-11                1,179,648\n",
              "│    │    └─LayerNorm: 3-12              1,536\n",
              "│    └─EncoderLayer: 2-8                 --\n",
              "│    │    └─MultiHeadAttention: 3-13     2,359,296\n",
              "│    │    └─Adapter: 3-14                1,179,648\n",
              "│    │    └─LayerNorm: 3-15              1,536\n",
              "│    │    └─FeedForwardNetwork: 3-16     4,722,432\n",
              "│    │    └─Adapter: 3-17                1,179,648\n",
              "│    │    └─LayerNorm: 3-18              1,536\n",
              "│    └─EncoderLayer: 2-9                 --\n",
              "│    │    └─MultiHeadAttention: 3-19     2,359,296\n",
              "│    │    └─Adapter: 3-20                1,179,648\n",
              "│    │    └─LayerNorm: 3-21              1,536\n",
              "│    │    └─FeedForwardNetwork: 3-22     4,722,432\n",
              "│    │    └─Adapter: 3-23                1,179,648\n",
              "│    │    └─LayerNorm: 3-24              1,536\n",
              "│    └─EncoderLayer: 2-10                --\n",
              "│    │    └─MultiHeadAttention: 3-25     2,359,296\n",
              "│    │    └─Adapter: 3-26                1,179,648\n",
              "│    │    └─LayerNorm: 3-27              1,536\n",
              "│    │    └─FeedForwardNetwork: 3-28     4,722,432\n",
              "│    │    └─Adapter: 3-29                1,179,648\n",
              "│    │    └─LayerNorm: 3-30              1,536\n",
              "│    └─EncoderLayer: 2-11                --\n",
              "│    │    └─MultiHeadAttention: 3-31     2,359,296\n",
              "│    │    └─Adapter: 3-32                1,179,648\n",
              "│    │    └─LayerNorm: 3-33              1,536\n",
              "│    │    └─FeedForwardNetwork: 3-34     4,722,432\n",
              "│    │    └─Adapter: 3-35                1,179,648\n",
              "│    │    └─LayerNorm: 3-36              1,536\n",
              "│    └─EncoderLayer: 2-12                --\n",
              "│    │    └─MultiHeadAttention: 3-37     2,359,296\n",
              "│    │    └─Adapter: 3-38                1,179,648\n",
              "│    │    └─LayerNorm: 3-39              1,536\n",
              "│    │    └─FeedForwardNetwork: 3-40     4,722,432\n",
              "│    │    └─Adapter: 3-41                1,179,648\n",
              "│    │    └─LayerNorm: 3-42              1,536\n",
              "│    └─EncoderLayer: 2-13                --\n",
              "│    │    └─MultiHeadAttention: 3-43     2,359,296\n",
              "│    │    └─Adapter: 3-44                1,179,648\n",
              "│    │    └─LayerNorm: 3-45              1,536\n",
              "│    │    └─FeedForwardNetwork: 3-46     4,722,432\n",
              "│    │    └─Adapter: 3-47                1,179,648\n",
              "│    │    └─LayerNorm: 3-48              1,536\n",
              "│    └─EncoderLayer: 2-14                --\n",
              "│    │    └─MultiHeadAttention: 3-49     2,359,296\n",
              "│    │    └─Adapter: 3-50                1,179,648\n",
              "│    │    └─LayerNorm: 3-51              1,536\n",
              "│    │    └─FeedForwardNetwork: 3-52     4,722,432\n",
              "│    │    └─Adapter: 3-53                1,179,648\n",
              "│    │    └─LayerNorm: 3-54              1,536\n",
              "│    └─EncoderLayer: 2-15                --\n",
              "│    │    └─MultiHeadAttention: 3-55     2,359,296\n",
              "│    │    └─Adapter: 3-56                1,179,648\n",
              "│    │    └─LayerNorm: 3-57              1,536\n",
              "│    │    └─FeedForwardNetwork: 3-58     4,722,432\n",
              "│    │    └─Adapter: 3-59                1,179,648\n",
              "│    │    └─LayerNorm: 3-60              1,536\n",
              "│    └─EncoderLayer: 2-16                --\n",
              "│    │    └─MultiHeadAttention: 3-61     2,359,296\n",
              "│    │    └─Adapter: 3-62                1,179,648\n",
              "│    │    └─LayerNorm: 3-63              1,536\n",
              "│    │    └─FeedForwardNetwork: 3-64     4,722,432\n",
              "│    │    └─Adapter: 3-65                1,179,648\n",
              "│    │    └─LayerNorm: 3-66              1,536\n",
              "│    └─EncoderLayer: 2-17                --\n",
              "│    │    └─MultiHeadAttention: 3-67     2,359,296\n",
              "│    │    └─Adapter: 3-68                1,179,648\n",
              "│    │    └─LayerNorm: 3-69              1,536\n",
              "│    │    └─FeedForwardNetwork: 3-70     4,722,432\n",
              "│    │    └─Adapter: 3-71                1,179,648\n",
              "│    │    └─LayerNorm: 3-72              1,536\n",
              "├─Pooler: 1-3                            --\n",
              "│    └─Linear: 2-18                      590,592\n",
              "│    └─Tanh: 2-19                        --\n",
              "├─Linear: 1-4                            1,538\n",
              "=================================================================\n",
              "Total params: 137,403,650\n",
              "Trainable params: 137,403,650\n",
              "Non-trainable params: 0\n",
              "================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizer\n"
      ],
      "metadata": {
        "id": "Hb8Gx6YOYVUq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Copyright 2018 The Google AI Language Team Authors and The HugginFace Inc. team.\n",
        "\n",
        "\"\"\" Tokenization classes (It's exactly the same code as Google BERT code \"\"\"\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import collections\n",
        "import unicodedata\n",
        "import six\n",
        "\n",
        "\n",
        "def convert_to_unicode(text):\n",
        "    \"\"\"Converts `text` to Unicode (if it's not already), assuming utf-8 input.\"\"\"\n",
        "    if six.PY3:\n",
        "        if isinstance(text, str):\n",
        "            return text\n",
        "        elif isinstance(text, bytes):\n",
        "            return text.decode(\"utf-8\", \"ignore\")\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
        "\n",
        " #   elif six.PY2:\n",
        " #       if isinstance(text, str):\n",
        " #           return text.decode(\"utf-8\", \"ignore\")\n",
        " #       elif isinstance(text, unicode):\n",
        " #           return text\n",
        " #       else:\n",
        " #           raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"Not running on Python2 or Python 3?\")\n",
        "\n",
        "\n",
        "def printable_text(text):\n",
        "    \"\"\"Returns text encoded in a way suitable for print or `tf.logging`.\"\"\"\n",
        "\n",
        "    # These functions want `str` for both Python2 and Python3, but in one case\n",
        "    # it's a Unicode string and in the other it's a byte string.\n",
        "    if six.PY3:\n",
        "        if isinstance(text, str):\n",
        "            return text\n",
        "        elif isinstance(text, bytes):\n",
        "            return text.decode(\"utf-8\", \"ignore\")\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
        "#    elif six.PY2:\n",
        "#        if isinstance(text, str):\n",
        "#            return text\n",
        "#        elif isinstance(text, unicode):\n",
        "#            return text.encode(\"utf-8\")\n",
        "#        else:\n",
        "#            raise ValueError(\"Unsupported string type: %s\" % (type(text)))\n",
        "    else:\n",
        "        raise ValueError(\"Not running on Python2 or Python 3?\")\n",
        "\n",
        "\n",
        "def load_vocab(vocab_file):\n",
        "    \"\"\"Loads a vocabulary file into a dictionary.\"\"\"\n",
        "    vocab = collections.OrderedDict()\n",
        "    index = 0\n",
        "    with open(vocab_file, \"r\") as reader:\n",
        "        while True:\n",
        "            token = convert_to_unicode(reader.readline())\n",
        "            if not token:\n",
        "                break\n",
        "            token = token.strip()\n",
        "            vocab[token] = index\n",
        "            index += 1\n",
        "    return vocab\n",
        "\n",
        "\n",
        "def convert_tokens_to_ids(vocab, tokens):\n",
        "    \"\"\"Converts a sequence of tokens into ids using the vocab.\"\"\"\n",
        "    ids = []\n",
        "    for token in tokens:\n",
        "        ids.append(vocab[token])\n",
        "    return ids\n",
        "\n",
        "\n",
        "def whitespace_tokenize(text):\n",
        "    \"\"\"Runs basic whitespace cleaning and splitting on a peice of text.\"\"\"\n",
        "    text = text.strip()\n",
        "    if not text:\n",
        "        return []\n",
        "    tokens = text.split()\n",
        "    return tokens\n",
        "\n",
        "\n",
        "class FullTokenizer(object):\n",
        "    \"\"\"Runs end-to-end tokenziation.\"\"\"\n",
        "\n",
        "    def __init__(self, vocab_file, do_lower_case=True):\n",
        "        self.vocab = load_vocab(vocab_file)\n",
        "        self.basic_tokenizer = BasicTokenizer(do_lower_case=do_lower_case)\n",
        "        self.wordpiece_tokenizer = WordpieceTokenizer(vocab=self.vocab)\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        split_tokens = []\n",
        "        for token in self.basic_tokenizer.tokenize(text):\n",
        "            for sub_token in self.wordpiece_tokenizer.tokenize(token):\n",
        "                split_tokens.append(sub_token)\n",
        "\n",
        "        return split_tokens\n",
        "\n",
        "    def convert_tokens_to_ids(self, tokens):\n",
        "        return convert_tokens_to_ids(self.vocab, tokens)\n",
        "\n",
        "    def convert_to_unicode(self, text):\n",
        "        return convert_to_unicode(text)\n",
        "\n",
        "\n",
        "\n",
        "class BasicTokenizer(object):\n",
        "    \"\"\"Runs basic tokenization (punctuation splitting, lower casing, etc.).\"\"\"\n",
        "\n",
        "    def __init__(self, do_lower_case=True):\n",
        "        \"\"\"Constructs a BasicTokenizer.\n",
        "        Args:\n",
        "          do_lower_case: Whether to lower case the input.\n",
        "        \"\"\"\n",
        "        self.do_lower_case = do_lower_case\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        \"\"\"Tokenizes a piece of text.\"\"\"\n",
        "        text = convert_to_unicode(text)\n",
        "        text = self._clean_text(text)\n",
        "        orig_tokens = whitespace_tokenize(text)\n",
        "        split_tokens = []\n",
        "        for token in orig_tokens:\n",
        "            if self.do_lower_case:\n",
        "                token = token.lower()\n",
        "                token = self._run_strip_accents(token)\n",
        "            split_tokens.extend(self._run_split_on_punc(token))\n",
        "\n",
        "        output_tokens = whitespace_tokenize(\" \".join(split_tokens))\n",
        "        return output_tokens\n",
        "\n",
        "    def _run_strip_accents(self, text):\n",
        "        \"\"\"Strips accents from a piece of text.\"\"\"\n",
        "        text = unicodedata.normalize(\"NFD\", text)\n",
        "        output = []\n",
        "        for char in text:\n",
        "            cat = unicodedata.category(char)\n",
        "            if cat == \"Mn\":\n",
        "                continue\n",
        "            output.append(char)\n",
        "        return \"\".join(output)\n",
        "\n",
        "    def _run_split_on_punc(self, text):\n",
        "        \"\"\"Splits punctuation on a piece of text.\"\"\"\n",
        "        chars = list(text)\n",
        "        i = 0\n",
        "        start_new_word = True\n",
        "        output = []\n",
        "        while i < len(chars):\n",
        "            char = chars[i]\n",
        "            if _is_punctuation(char):\n",
        "                output.append([char])\n",
        "                start_new_word = True\n",
        "            else:\n",
        "                if start_new_word:\n",
        "                    output.append([])\n",
        "                start_new_word = False\n",
        "                output[-1].append(char)\n",
        "            i += 1\n",
        "\n",
        "        return [\"\".join(x) for x in output]\n",
        "\n",
        "    def _clean_text(self, text):\n",
        "        \"\"\"Performs invalid character removal and whitespace cleanup on text.\"\"\"\n",
        "        output = []\n",
        "        for char in text:\n",
        "            cp = ord(char)\n",
        "            if cp == 0 or cp == 0xfffd or _is_control(char):\n",
        "                continue\n",
        "            if _is_whitespace(char):\n",
        "                output.append(\" \")\n",
        "            else:\n",
        "                output.append(char)\n",
        "        return \"\".join(output)\n",
        "\n",
        "\n",
        "class WordpieceTokenizer(object):\n",
        "    \"\"\"Runs WordPiece tokenization.\"\"\"\n",
        "\n",
        "    def __init__(self, vocab, unk_token=\"[UNK]\", max_input_chars_per_word=100):\n",
        "        self.vocab = vocab\n",
        "        self.unk_token = unk_token\n",
        "        self.max_input_chars_per_word = max_input_chars_per_word\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        \"\"\"Tokenizes a piece of text into its word pieces.\n",
        "        This uses a greedy longest-match-first algorithm to perform tokenization\n",
        "        using the given vocabulary.\n",
        "        For example:\n",
        "          input = \"unaffable\"\n",
        "          output = [\"un\", \"##aff\", \"##able\"]\n",
        "        Args:\n",
        "          text: A single token or whitespace separated tokens. This should have\n",
        "            already been passed through `BasicTokenizer.\n",
        "        Returns:\n",
        "          A list of wordpiece tokens.\n",
        "        \"\"\"\n",
        "\n",
        "        text = convert_to_unicode(text)\n",
        "\n",
        "        output_tokens = []\n",
        "        for token in whitespace_tokenize(text):\n",
        "            chars = list(token)\n",
        "            if len(chars) > self.max_input_chars_per_word:\n",
        "                output_tokens.append(self.unk_token)\n",
        "                continue\n",
        "\n",
        "            is_bad = False\n",
        "            start = 0\n",
        "            sub_tokens = []\n",
        "            while start < len(chars):\n",
        "                end = len(chars)\n",
        "                cur_substr = None\n",
        "                while start < end:\n",
        "                    substr = \"\".join(chars[start:end])\n",
        "                    if start > 0:\n",
        "                        substr = \"##\" + substr\n",
        "                    if substr in self.vocab:\n",
        "                        cur_substr = substr\n",
        "                        break\n",
        "                    end -= 1\n",
        "                if cur_substr is None:\n",
        "                    is_bad = True\n",
        "                    break\n",
        "                sub_tokens.append(cur_substr)\n",
        "                start = end\n",
        "\n",
        "            if is_bad:\n",
        "                output_tokens.append(self.unk_token)\n",
        "            else:\n",
        "                output_tokens.extend(sub_tokens)\n",
        "        return output_tokens\n",
        "\n",
        "\n",
        "def _is_whitespace(char):\n",
        "    \"\"\"Checks whether `chars` is a whitespace character.\"\"\"\n",
        "    # \\t, \\n, and \\r are technically contorl characters but we treat them\n",
        "    # as whitespace since they are generally considered as such.\n",
        "    if char == \" \" or char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
        "        return True\n",
        "    cat = unicodedata.category(char)\n",
        "    if cat == \"Zs\":\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def _is_control(char):\n",
        "    \"\"\"Checks whether `chars` is a control character.\"\"\"\n",
        "    # These are technically control characters but we count them as whitespace\n",
        "    # characters.\n",
        "    if char == \"\\t\" or char == \"\\n\" or char == \"\\r\":\n",
        "        return False\n",
        "    cat = unicodedata.category(char)\n",
        "    if cat.startswith(\"C\"):\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def _is_punctuation(char):\n",
        "    \"\"\"Checks whether `chars` is a punctuation character.\"\"\"\n",
        "    cp = ord(char)\n",
        "    # We treat all non-letter/number ASCII as punctuation.\n",
        "    # Characters such as \"^\", \"$\", and \"`\" are not in the Unicode\n",
        "    # Punctuation class but we treat them as punctuation anyways, for\n",
        "    # consistency.\n",
        "    if ((cp >= 33 and cp <= 47) or (cp >= 58 and cp <= 64) or\n",
        "            (cp >= 91 and cp <= 96) or (cp >= 123 and cp <= 126)):\n",
        "        return True\n",
        "    cat = unicodedata.category(char)\n",
        "    if cat.startswith(\"P\"):\n",
        "        return True\n",
        "    return False"
      ],
      "metadata": {
        "id": "EYLHAPo7uDIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fire"
      ],
      "metadata": {
        "id": "HaR1Z1_tV_-o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61d89796-0d1b-42ee-aa1d-5fd3967e03c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fire\n",
            "  Downloading fire-0.4.0.tar.gz (87 kB)\n",
            "\u001b[?25l\r\u001b[K     |███▊                            | 10 kB 19.1 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 20 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 30 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 40 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 51 kB 3.7 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 61 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 71 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 81 kB 4.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 87 kB 2.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from fire) (1.15.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from fire) (1.1.0)\n",
            "Building wheels for collected packages: fire\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115942 sha256=ab1d977bcbde3ed71c5cdd2febf231e539c65d844b3e53a3b45f883d658da6bc\n",
            "  Stored in directory: /root/.cache/pip/wheels/8a/67/fb/2e8a12fa16661b9d5af1f654bd199366799740a85c64981226\n",
            "Successfully built fire\n",
            "Installing collected packages: fire\n",
            "Successfully installed fire-0.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare dataset\n"
      ],
      "metadata": {
        "id": "0jsGQUUSXJpd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset"
      ],
      "metadata": {
        "id": "wtgLiE1aYflG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = FullTokenizer(vocab_file='vocab.txt', do_lower_case=True)"
      ],
      "metadata": {
        "id": "erwveecyZacH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Pipeline():\n",
        "    \"\"\" Preprocess Pipeline Class : callable \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def __call__(self, instance):\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "class Tokenizing(Pipeline):\n",
        "    \"\"\" Tokenizing sentence pair \"\"\"\n",
        "    def __init__(self, preprocessor, tokenize):\n",
        "        super().__init__()\n",
        "        self.preprocessor = preprocessor # e.g. text normalization\n",
        "        self.tokenize = tokenize # tokenize function\n",
        "\n",
        "    def __call__(self, instance):\n",
        "        label, text_a = instance\n",
        "\n",
        "        label = self.preprocessor(label)\n",
        "        tokens_a = self.tokenize(self.preprocessor(text_a))\n",
        "\n",
        "        return (label, tokens_a)"
      ],
      "metadata": {
        "id": "F4eUw1Egg5Bw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMl1NwyTiHUy",
        "outputId": "a7290e17-03dc-4970-e141-9c1fce0d31ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting txt\n",
            "  Downloading txt-2020.11.12-py3-none-any.whl (3.8 kB)\n",
            "Installing collected packages: txt\n",
            "Successfully installed txt-2020.11.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "class CsvDataset(Dataset):\n",
        "    \"\"\" Dataset Class for CSV file \"\"\"\n",
        "    labels = None\n",
        "    def __init__(self, file, pipeline=[]): # cvs file and pipeline object\n",
        "        Dataset.__init__(self)\n",
        "        data = []\n",
        "        with open(file, \"r\") as f:\n",
        "            # list of splitted lines : line is also list\n",
        "            lines = csv.reader(f, delimiter=',')\n",
        "            for instance in self.get_instances(lines): # instance : tuple of fields\n",
        "                for proc in pipeline: # a bunch of pre-processing\n",
        "                    instance = proc(instance)\n",
        "                #print(len(instance[0]))\n",
        "                data.append(instance)\n",
        "\n",
        "        \n",
        "\n",
        "        # To Tensors\n",
        "        self.tensors = [torch.tensor(x, dtype=torch.long) for x in zip(*data)]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.tensors[0].size(0)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return tuple(tensor[index] for tensor in self.tensors)\n",
        "\n",
        "    def get_instances(self, lines):\n",
        "        \"\"\" get instance array from (csv-separated) line list \"\"\"\n",
        "        raise "
      ],
      "metadata": {
        "id": "Veobun0ghmDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools"
      ],
      "metadata": {
        "id": "_7BHvqD4it_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MRPC(CsvDataset):\n",
        "    \"\"\" Dataset class for MRPC \"\"\"\n",
        "    labels = (\"ham\", \"spam\") # label names\n",
        "    def __init__(self, file, pipeline=[]):\n",
        "        super().__init__(file, pipeline)\n",
        "\n",
        "    def get_instances(self, lines):\n",
        "        for line in lines: # skip header\n",
        "            yield line[0], line[1]   # label, text_a, text_b"
      ],
      "metadata": {
        "id": "X9wXZrdniZhG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dataset_class(task):\n",
        "    \"\"\" Mapping from task string to Dataset Class \"\"\"\n",
        "    table = {'mrpc': MRPC}\n",
        "    return table[task]"
      ],
      "metadata": {
        "id": "Sq18S2o-iyzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AddSpecialTokensWithTruncation(Pipeline):\n",
        "    \"\"\" Add special tokens [CLS], [SEP] with truncation \"\"\"\n",
        "    def __init__(self, max_len=50):\n",
        "        super().__init__()\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __call__(self, instance):\n",
        "        label, tokens_a= instance\n",
        "\n",
        "        # -3 special tokens for [CLS] text_a [SEP] text_b [SEP]\n",
        "        # -2 special tokens for [CLS] text_a [SEP]\n",
        "        _max_len = self.max_len - 2\n",
        "        # Add Special Tokens\n",
        "        if len(tokens_a)>_max_len:\n",
        "          tokens_a=tokens_a[:_max_len]\n",
        "        tokens_a = ['[CLS]'] + tokens_a + ['[SEP]']\n",
        "        #print(len(tokens_a))\n",
        "        return (label, tokens_a)"
      ],
      "metadata": {
        "id": "Jbxi3M4A8Za_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenIndexing(Pipeline):\n",
        "    \"\"\" Convert tokens into token indexes and do zero-padding \"\"\"\n",
        "    def __init__(self, indexer, labels, max_len=512):\n",
        "        super().__init__()\n",
        "        self.indexer = indexer # function : tokens to indexes\n",
        "        # map from a label name to a label index\n",
        "        self.label_map = {name: i for i, name in enumerate(labels)}\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __call__(self, instance):\n",
        "        label, tokens_a= instance\n",
        "\n",
        "        input_ids = self.indexer(tokens_a )\n",
        "        segment_ids = [0]*len(tokens_a) # token type ids\n",
        "        input_mask = [1]*len(tokens_a)\n",
        "\n",
        "        label_id = self.label_map[label]\n",
        "\n",
        "      #  print(self.max_len)\n",
        "        # zero padding\n",
        "        n_pad = self.max_len - len(input_ids)\n",
        "        input_ids.extend([0]*n_pad)\n",
        "        segment_ids.extend([0]*n_pad)\n",
        "        input_mask.extend([0]*n_pad)\n",
        "      #  print(len(input_ids))\n",
        "      #  print(len(segment_ids))\n",
        "      #  print(len(input_mask))\n",
        "\n",
        "        return (input_ids, segment_ids, input_mask, label_id)"
      ],
      "metadata": {
        "id": "ea8-y5Vx2jz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "task='mrpc'\n",
        "data_file='SMSSpamCollection.csv'"
      ],
      "metadata": {
        "id": "XQwD0R4KjFMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_len=50\n",
        "TaskDataset = dataset_class(task)\n",
        "\n",
        "pipeline = [Tokenizing(tokenizer.convert_to_unicode, tokenizer.tokenize),\n",
        "            AddSpecialTokensWithTruncation(max_len),\n",
        "            TokenIndexing(tokenizer.convert_tokens_to_ids,TaskDataset.labels, max_len)]\n",
        "dataset = TaskDataset(data_file, pipeline)"
      ],
      "metadata": {
        "id": "nP4HrtQWU6R3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "ZEAy1UKNk056"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_iter = DataLoader(dataset, batch_size=20, shuffle=True)"
      ],
      "metadata": {
        "id": "uQOWce2vlAWO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classifier"
      ],
      "metadata": {
        "id": "urn0qoJhlzPx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install loralib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UlIWPuIK3tNn",
        "outputId": "0d78beaf-49ce-4f18-c7be-c193e19ec15f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting loralib\n",
            "  Downloading loralib-0.1.1-py3-none-any.whl (8.8 kB)\n",
            "Installing collected packages: loralib\n",
            "Successfully installed loralib-0.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = adaptermodel\n",
        "lr = 1e-3\n",
        "epochs = 500\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# 冻结fc1层的参数\n",
        "for name, param in model.named_parameters():\n",
        "  if \"adapter\" in name:\n",
        "    param.requires_grad = False\n",
        "\n",
        "\n",
        "optimizer = Adadelta(filter(lambda p : p.requires_grad, model.parameters()), lr=lr)\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# training\n",
        "for epoch in range(epochs):\n",
        "    for one_batch in data_iter:\n",
        "        input_ids, segment_ids, masked_tokens,is_next = [ele.to(device) for ele in one_batch]\n",
        "        print(input_ids.shape)\n",
        "        \n",
        "        logits_cls= model(input_ids, segment_ids,masked_tokens)\n",
        "        loss_cls = criterion(logits_cls, is_next)\n",
        "        loss = loss_cls\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f'Epoch:{epoch + 1} \\t loss: {loss:.6f}')\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "metadata": {
        "id": "rTRmJeB8OyA1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}